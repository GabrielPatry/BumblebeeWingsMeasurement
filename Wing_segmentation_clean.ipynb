{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a415af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing useful libraries\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import itertools\n",
    "import matplotlib.colors as mcolors\n",
    "import io\n",
    "from PIL import Image\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import imageio\n",
    "import time\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
    "from scipy.ndimage import rotate\n",
    "from scipy.optimize import minimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "913fa5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some  global variables\n",
    "\n",
    "MAX_IMAGE_SIZE = 3000000 #this et the maximum number of pixels we authorize for an image\n",
    "#this has to be lowered in case of GPUOutOfMemory error \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea8acab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some useful functions\n",
    "\n",
    "def list_directory_contents(directory):\n",
    "    \"\"\"return a list of all directories and folder inside directory\"\"\"\n",
    "    pictures_list = []\n",
    "\n",
    "    try:\n",
    "        with os.scandir(directory) as entries:\n",
    "            for entry in entries:\n",
    "                #print(entry.name)\n",
    "                pictures_list.append(entry.name)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The directory {directory} does not exist.\")\n",
    "    except NotADirectoryError:\n",
    "        print(f\"{directory} is not a directory.\")\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied to access {directory}.\")\n",
    "    return pictures_list\n",
    "\n",
    "def list_files(startpath,exclude = \"Vrac\"):\n",
    "    \"\"\"From a stratpath this function list all files with full directory\n",
    "    We allow to exclude one folder (can be easily adapted to several)\"\"\"\n",
    "    file_list = []\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        if not exclude in root:\n",
    "            for file in files:\n",
    "                file_list.append(os.path.join(root, file))\n",
    "    return file_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c6a3d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (image_encoder): ImageEncoderViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-23): 24 x Block(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): Sequential(\n",
       "      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): LayerNorm2d()\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): LayerNorm2d()\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): PromptEncoder(\n",
       "    (pe_layer): PositionEmbeddingRandom()\n",
       "    (point_embeddings): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "    (mask_downscaling): Sequential(\n",
       "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): LayerNorm2d()\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): MaskDecoder(\n",
       "    (transformer): TwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TwoWayAttentionBlock(\n",
       "          (self_attn): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): Attention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (output_upscaling): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): GELU(approximate='none')\n",
       "    )\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading SAM\n",
    "\n",
    "model_type = \"vit_l\" #is hace more or less gpu memory, you can switch to less or more heavy models, see http://github.com/facebookresearch/segment-anything for more details and checkpoint download\n",
    "sam = sam_model_registry[model_type](checkpoint = '/home/gabriel/Documents/TR DIMA/codes/sam_checkpoint/sam_vit_l_0b3195.pth') #put here the path to checkpoint\n",
    "sam.cuda() #moving model to cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995fc862",
   "metadata": {},
   "source": [
    "**Section 1 :** \n",
    "Image cropping $ \\\\ $\n",
    "(you must restart the kernel each time you wan't to lauch the code of this section to avoid GPUoutofmemoryerror)\n",
    " $ \\\\ $\n",
    " You may have issues with GPU memory if you lauch the cell with sam.cuda() instruction several time during the same session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5a5d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating AUtomaticMaskGenerator instance\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(sam,min_mask_region_area=5000)\n",
    "\n",
    "#the second parameter stand to avoid masks with several connected components with one of them to small to be a wing\n",
    "#he might be ajusted if your dataset have very high quality or very low quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d1260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful functions for section 1\n",
    "\n",
    "def Find_bbox(mask):\n",
    "    \"\"\"return x_min,y_min,x_max,y_max for the mask\"\"\"\n",
    "    mask = np.array(mask,dtype = np.uint8)\n",
    "    mask_coords = np.argwhere(mask == 1)\n",
    "    xmin,ymin,xmax,ymax = np.min(mask_coords[:,0]),np.min(mask_coords[:,1]),np.max(mask_coords[:,0]),np.max(mask_coords[:,1])\n",
    "    return xmin,ymin,xmax,ymax\n",
    "\n",
    "def Rotation_matrix(theta):\n",
    "\n",
    "    return np.array([[np.cos(theta), -np.sin(theta)],\n",
    "                     [np.sin(theta), np.cos(theta)]])\n",
    "\n",
    "\n",
    "def Find_Optimal_Rotation(mask):\n",
    "\n",
    "    mask = np.array(mask,dtype = np.uint8)\n",
    "\n",
    "    where_array = np.argwhere(mask == 1)\n",
    "\n",
    "    def objective_function(theta):\n",
    "        R_m = Rotation_matrix(theta)\n",
    "        rotated_mask_coordinates = np.dot(where_array,R_m)\n",
    "        xmin,ymin,xmax,ymax = np.min(rotated_mask_coordinates[:,0]),np.min(rotated_mask_coordinates[:,1]),np.max(rotated_mask_coordinates[:,0]),np.max(rotated_mask_coordinates[:,1])\n",
    "        width = ymax - ymin\n",
    "        height = xmax - xmin\n",
    "        return -width/height\n",
    "    \n",
    "    result = minimize(objective_function,x0 = 0.0,method='BFGS',tol = 0.0001)\n",
    "    best_angle = np.squeeze(result.x,axis = 0)\n",
    "    best_ratio = np.abs(result.fun)\n",
    "    return best_angle,best_ratio\n",
    "\n",
    "def Universal_crop_reshaper(wing_picture,wing_mask,goal_shape = (256,512)):\n",
    "    \"\"\"return the reshape crop of wing with boundarie adjustment to fit goal_shape\n",
    "     without any deformations \"\"\"\n",
    "    xmin_i,ymin_i,xmax_i,ymax_i = Find_bbox(wing_mask)\n",
    "    x,y = goal_shape  \n",
    "\n",
    "    ratio_ini = (ymax_i-ymin_i)/(xmax_i-xmin_i)\n",
    "    ratio_f = y/x\n",
    "\n",
    "\n",
    "    if ratio_f > ratio_ini:\n",
    "        ymin_f = int(max(0,(ymax_i+ymin_i)/2-ratio_f/ratio_ini*(ymax_i-ymin_i)/2))\n",
    "        ymax_f = int(ymin_f + (ymax_i-ymin_i)*ratio_f/ratio_ini)\n",
    "        xmin_f,xmax_f  = xmin_i,xmax_i\n",
    "    else:\n",
    "        xmin_f = int(max(0,(xmax_i+xmin_i)/2-ratio_ini/ratio_f*(xmax_i-xmin_i)/2))\n",
    "        xmax_f = int(xmin_f + (xmax_i-xmin_i)*ratio_ini/ratio_f)\n",
    "        ymin_f,ymax_f  = ymin_i,ymax_i\n",
    "\n",
    "    cropped_wing = wing_picture[xmin_f:xmax_f,ymin_f:ymax_f]\n",
    "    resized_cropped_wing = cv2.resize(cropped_wing,(y,x))\n",
    "    return resized_cropped_wing\n",
    "\n",
    "def segment_and_crop(wing_image,option = 'C'):\n",
    "    \"\"\"this function take image with wing and white background\n",
    "      as argument and return segmented and cropped wing as np array\n",
    "      \"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()  #clear gpu memory (don' t know if it's really useful)\n",
    "    \n",
    "    xi,yi,zi = wing_image.shape\n",
    "    reduction_factor = max(1,(xi*yi/MAX_IMAGE_SIZE)**(1/2)) \n",
    "    #the parameter above is necessary, because some images are too big, and this cause issues for SAM inference\n",
    "    image_r = cv2.resize(wing_image, (int(yi/reduction_factor), int(xi/reduction_factor)))\n",
    "\n",
    "    # Generate masks\n",
    "    with torch.no_grad():\n",
    "        masks = mask_generator.generate(image_r) #generating masks\n",
    "\n",
    "    current_max_area = 0   #this will store the current area of the object that verify the condition below : the goal is to extract the largest one that respect that condition\n",
    "    wing_mask = None\n",
    "    effective_best_rotation = 0 #in case something went wrong, we set default best rotation to 0\n",
    "    effective_best_ratio = None\n",
    "\n",
    "    #thanks to the parameters set for AutomaticMaskGenerator, we don't need a filter for largest connected component\n",
    "    #we however have to compute the rotation that maximize width/height factor, to use this to discriminate masks\n",
    "    #this will be then used to rotate wings\n",
    "    \n",
    "\n",
    "\n",
    "    for mask in masks:\n",
    "        effective_mask = np.array(mask['segmentation'],dtype = np.uint8)\n",
    "        best_rotation,best_ratio = Find_Optimal_Rotation(effective_mask)\n",
    "\n",
    "        if 2.2<best_ratio<2.8: #parameters set empirically\n",
    "            if mask['area'] > current_max_area:\n",
    "                effective_best_rotation = best_rotation\n",
    "                effective_best_ratio = best_ratio #this variable is never used\n",
    "                wing_mask = mask\n",
    "                current_max_area = mask['area'] #update current_max_area value\n",
    "\n",
    "\n",
    "    effective_best_rotation = effective_best_rotation  * 180/np.pi #conversion to degree\n",
    "    effective_mask = rotate(np.array(wing_mask['segmentation'],dtype = np.float32),angle=effective_best_rotation,reshape = True)\n",
    "    effective_mask = np.round(effective_mask).astype(np.uint8)\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    wing_image_rotated = rotate(image_r,angle=effective_best_rotation,reshape = True)\n",
    "\n",
    "    if option == 'C':\n",
    "        xmin,ymin,xmax,ymax = Find_bbox(effective_mask)\n",
    "        cropped_rotated_wing = wing_image_rotated[xmin:xmax,ymin:ymax]\n",
    "        cropped_rotated_wing = Universal_crop_reshaper(wing_image_rotated,effective_mask)\n",
    "    elif option == 'S':\n",
    "        xmin,ymin,xmax,ymax = Find_bbox(effective_mask)\n",
    "        effective_mask = np.expand_dims(effective_mask,axis = 2).astype(np.float32)\n",
    "        cropped_rotated_wing = wing_image_rotated[xmin:xmax,ymin:ymax]*effective_mask[xmin:xmax,ymin:ymax]\n",
    "\n",
    "\n",
    "    return cropped_rotated_wing\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf2399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening and processing images\n",
    "#run this cell if you want to test the above protocol \"on the fly\", or test it on specific images\n",
    "\n",
    "wing_images_directory = \"/home/gabriel/Documents/TR DIMA/data/Images_orga_cropping\"\n",
    "\n",
    "image_names = list_files(wing_images_directory)\n",
    "test_name = \"/home/gabriel/Documents/TR DIMA/data/Images_orga_cropping/Bombus_ruderarius/Ouvriere/ABAURA4902/ABAURA4902_S3.jpg\"\n",
    "\n",
    "images_test = image_names[900:905] #you can for instance take some images in your images folder\n",
    "\n",
    "for dir in images_test:\n",
    "    current_image = cv2.imread(dir)\n",
    "    current_image = cv2.cvtColor(current_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    current_crop = segment_and_crop(current_image,option='C')\n",
    "\n",
    "    plt.imshow(current_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    plt.imshow(current_crop)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e73220",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cropping images and saving them into a specific folder\n",
    "\n",
    "wing_images_directory = \"/home/gabriel/Documents/TR DIMA/data/Images_orga_cropping\"\n",
    "\n",
    "list_file_names = list_files(wing_images_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
